{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"dataset.csv\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings # Settings: Là một đối tượng chứa các thiết lập chung mà framework LlamaIndex sử dụng để làm việc với các mô-đun khác\n",
    "from llama_index.llms.openai import OpenAI # OpenAI: Đây là lớp cung cấp khả năng kết nối và gọi các mô hình ngôn ngữ (LLMs) từ OpenAI, ví dụ như gpt-3.5-turbo hoặc gpt-4\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding #OpenAIEmbedding: Đây là lớp để sử dụng mô hình nhúng từ OpenAI, giúp chuyển đổi văn bản thành dạng vector số (embeddings) để phục vụ tìm kiếm hoặc các tác vụ khác.\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\") #Settings.llm: Gắn một mô hình xử lý ngôn ngữ (language model) vào framework, ở đây là gpt-3.5-turbo từ OpenAI.\n",
    "#LlamaIndex sẽ sử dụng mô hình ngôn ngữ này cho mọi tác vụ yêu cầu tạo văn bản hoặc tương tác với AI, như sinh câu trả lời hoặc xử lý câu hỏi\n",
    "\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\") # Thiết lập mô hình nhúng để chuyển văn bản thành vector số, Ở đây sử dụng mô hình text-embedding-ada-002 từ OpenAI.\n",
    "# Tìm kiếm thông tin dựa trên ngữ nghĩa.\n",
    "# So khớp hoặc phân nhóm các văn bản dựa trên độ tương đồng vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Summary Index and Vector Index over the Same Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "#SummaryIndex sử dụng mô hình ngôn ngữ để đọc qua nội dung của các nodes, tạo ra các bản tóm tắt. Khi có một truy vấn, nó sẽ dựa vào thông tin tóm tắt để trả lời câu hỏi hoặc cung cấp nội dung cô đọng\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "# Sử dụng một mô hình nhúng để chuyển các nodes thành vector số. Khi nhận được một truy vấn, nó cũng được chuyển thành vector số. Sau đó, quá trình tìm kiếm sẽ dựa trên độ tương đồng giữa các vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Query Engines and Set Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# khởi tạo các Query Engines (công cụ truy vấn) từ hai đối tượng summary_index và vector_index\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ") #được dùng để xây dựng công cụ truy vấn cho mục đích tóm tắt.\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "# công cụ truy vấn liên quan đến xử lý hoặc tìm kiếm dữ liệu theo không gian vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "#định nghĩa hai công cụ truy vấn (QueryEngineTool) dựa trên \n",
    "# các query engine đã được khởi tạo trước đó (summary_query_engine và vector_query_engine) và thêm mô tả cụ thể để giải thích mục đích sử dụng của từng công cụ\n",
    "\n",
    "#  Tạo công cụ tóm tắt:  công cụ truy vấn được thiết kế để trả lời các câu hỏi yêu cầu tóm tắt\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Presight\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Tạo công cụ tìm kiếm ngữ cảnh: công cụ truy vấn chuyên dụng cho các câu hỏi yêu cầu tìm kiếm thông tin chi tiết từ tài liệu MetaGPT\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the Presight web.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "#  tạo công cụ truy vấn có thể định tuyến các truy vấn đến các \"Query Engine Tool\" phù hợp\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(), #Một lớp để chọn công cụ truy vấn (Query Engine Tool) dựa trên trí tuệ nhân tạo ngôn ngữ (LLM).\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True#Khi được bật, chế độ này cung cấp thông tin chi tiết trong quá trình định tuyến và thực thi truy vấn, chẳng hạn như thông báo về việc công cụ nào được chọn.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The choice mentions summarization questions related to Presight, which aligns with the task of summarizing Presight..\n",
      "\u001b[0mPresight is an AI-powered data platform that is committed to protecting the privacy of its customers and website visitors. The platform collects various types of information such as email addresses, names, phone numbers, addresses, cookies, and usage data for the purpose of providing and improving its services. Users have the right to access, correct, and amend their personal information through the application. Presight uses automated edit checks to ensure data accuracy and integrity. The collected data is used for maintaining the service, notifying users of changes, providing customer support, analyzing for improvements, monitoring usage, and addressing technical issues. Personal information is not retained or used for developing generalized AI models. Data security measures include encryption, regular security audits, and employee training. The platform is committed to data retention and disposal practices, relying on data subjects for accurate information. Presight uses cookies to enhance user experience and monitors data processing activities for compliance with privacy policies and data protection laws. In case of a data breach, users will be notified, and authorities will be informed as required by law. The platform cooperates with data protection authorities and updates its privacy policy periodically.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"summary presight\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Auto-Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=1,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query,\n",
    "    description=\"Useful for retrieving specific context from the Presight web..\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Presight\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Privacy policy\"}\n",
      "=== Function Output ===\n",
      "Presight is dedicated to safeguarding the privacy of its customers and website visitors by explaining how information is collected, used, and disclosed. Personal data collected includes email addresses, names, phone numbers, addresses, cookies, and usage data. Users have the right to access and edit their personal information. Presight uses collected data for various purposes such as maintaining services, notifying users of changes, providing customer support, and improving services. The platform does not retain or use Google User Data for AI/ML model development and ensures data security through encryption and regular security audits. Data retention is based on active account status, and users are responsible for providing accurate information. The privacy policy is periodically updated, and users can control cookie usage through browser settings. Third-party websites linked on the platform are not under Presight's privacy practices.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is PRIVACY POLICY?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"paper\"}\n",
      "=== Function Output ===\n",
      "The entity is committed to using personal information only for the purposes identified in its privacy policy.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "   \"What is a summary of the paper?\",\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
